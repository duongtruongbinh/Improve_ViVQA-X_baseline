{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a37ac0a",
   "metadata": {},
   "source": [
    "### Detect obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75611c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "# Load the OWLv2 processor and model once at module load\n",
    "\n",
    "\n",
    "def detect_object(\n",
    "    image: Image.Image,\n",
    "    class_name: str\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Tool: DetectObject – Returns bounding boxes for all instances of the given class name,\n",
    "    in [x, y, width, height] (Pascal VOC → xywh) format.\n",
    "\n",
    "    Parameters:\n",
    "        image: A PIL Image to run detection on.\n",
    "        class_name: The target class to detect (e.g. \"cat\", \"dog\").\n",
    "\n",
    "    Returns:\n",
    "        A list of bounding boxes [x, y, width, height], one list per detected instance.\n",
    "    \"\"\"\n",
    "    _processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "    _model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "\n",
    "    # Prepare the grounding query\n",
    "    text_labels = [[f\"a photo of a {class_name}\"]]\n",
    "    inputs = _processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "    outputs = _model(**inputs)\n",
    "\n",
    "    # Map normalized predictions back to pixel coordinates\n",
    "    target_sizes = torch.tensor([(image.height, image.width)])\n",
    "    results = _processor.post_process_grounded_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=0.1,\n",
    "        text_labels=text_labels\n",
    "    )\n",
    "\n",
    "    # Extract boxes for the first query\n",
    "    result = results[0]\n",
    "    pascal_boxes = result[\"boxes\"]  # Tensor of shape (N, 4): xmin,ymin,xmax,ymax\n",
    "\n",
    "    # Convert each Pascal VOC box to [x, y, width, height]\n",
    "    xywh_boxes: List[List[float]] = []\n",
    "    for box in pascal_boxes:\n",
    "        xmin, ymin, xmax, ymax = box.tolist()\n",
    "        width  = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        xywh_boxes.append([xmin, ymin, width, height])\n",
    "\n",
    "    return xywh_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a7959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"../cat.jpg\").convert(\"RGB\")\n",
    "dog_boxes = detect_object(img, \"cat\")\n",
    "for bbox in dog_boxes:\n",
    "    x, y, w, h = bbox\n",
    "    print(f\"Detected cat at x={x}, y={y}, width={w}, height={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eacc15",
   "metadata": {},
   "source": [
    "### Decompose question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3e6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Annotated\n",
    "from langchain_google_vertexai import VertexAI\n",
    "import os, json\n",
    "from google.cloud import aiplatform\n",
    "from autogen_core.tools import FunctionTool\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize Vertex AI\n",
    "CREDENTIALS_PATH = \"../credentials/bwa-agents-ad0e1f5ab4b7.json\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(CREDENTIALS_PATH)\n",
    "aiplatform.init(project='bwa-agents', location='us-central1')\n",
    "\n",
    "\n",
    "def decompose_question(\n",
    "    question: Annotated[str, \"Complex question to decompose\"]\n",
    ") -> List[Annotated[str, \"Simpler sub-question\"]]:\n",
    "    \"\"\"\n",
    "    Tool: DecomposeQuestion – Splits a complex question into two simpler sub-questions.\n",
    "    \"\"\"\n",
    "    llm = VertexAI(model_name=\"gemini-2.0-flash-lite-001\")\n",
    "    prompt = (\n",
    "            f\"Decompose the following complex question into exactly two simpler, self-contained sub-questions. \"\n",
    "            f\"The sub-questions, when answered sequentially, should help answer the original complex question. \"\n",
    "            f\"Return the two sub-questions as a JSON array of strings.\\n\\n\"\n",
    "            f\"Complex Question: \\\"What is the Köppen climate classification for the city where this mosque is located?\\\"\\n\"\n",
    "            f\"Output: [\\\"In which city is this mosque located?\\\", \\\"What is the Köppen climate classification for this city?\\\"]\\n\\n\"\n",
    "            f\"Complex Question: \\\"Who is the CEO of the company that developed the game featuring a plumber who jumps on turtles?\\\"\\n\"\n",
    "            f\"Output: [\\\"Which company developed the game featuring a plumber who jumps on turtles?\\\", \\\"Who is the CEO of that company?\\\"]\\n\\n\"\n",
    "            f\"Complex Question: \\\"{question}\\\"\\n\"\n",
    "            f\"Output:\"\n",
    "        )\n",
    "    resp = llm.invoke(prompt)\n",
    "    try:\n",
    "        # Expecting JSON array\n",
    "        result = json.loads(resp)\n",
    "        if isinstance(result, list) and len(result) == 2:\n",
    "            return result  # type: ignore\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # Fallback: take first two non-empty lines\n",
    "    lines = [ln.strip(' -\"') for ln in resp.splitlines() if ln.strip()]\n",
    "    return lines[:2]\n",
    "\n",
    "examples = [\n",
    "        \"What is the tallest mountain in the country where the world’s largest waterfall is located?\",\n",
    "        \"Who is the author of the novel that inspired the movie about a girl who shrinks to the size of an ant?\",\n",
    "        \"How many goals did the top scorer of the 2022 World Cup score, and which country did he play for?\"\n",
    "    ]\n",
    "\n",
    "for idx, q in enumerate(examples, 1):\n",
    "    print(f\"\\nExample #{idx}:\")\n",
    "    print(\"Original question:\", q)\n",
    "    try:\n",
    "        subs = decompose_question(q)\n",
    "        print(\"Sub-questions:\")\n",
    "        for i, sub in enumerate(subs, 1):\n",
    "            print(f\"  {i}. {sub}\")\n",
    "    except Exception as e:\n",
    "            print(\"Error calling decompose_question:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minhtq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
